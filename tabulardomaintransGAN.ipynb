{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the synthetic dataset for schizophrenia clinical trials\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Parameters & Configurations ---\n",
    "\n",
    "# --- Regional Placebo Effects ---\n",
    "PLACEBO_PANSS_CHANGE_MEAN_EU = -4.0\n",
    "PLACEBO_PANSS_CHANGE_MEAN_US = -15.0\n",
    "PLACEBO_PANSS_CHANGE_STD_EU = 2.0\n",
    "PLACEBO_PANSS_CHANGE_STD_US = 4.0\n",
    "\n",
    "# --- Regional Demographic Parameters ---\n",
    "ETHNICITY_CATEGORIES = ['White', 'Black', 'Hispanic/Latino', 'Asian', 'Other/Mixed']\n",
    "ETHNICITY_WEIGHTS_US = [0.60, 0.15, 0.15, 0.06, 0.04]\n",
    "ETHNICITY_WEIGHTS_EU = [0.88, 0.03, 0.01, 0.03, 0.05]\n",
    "COMORBIDITY_SUBSTANCE_USE_RATE_US = 0.40\n",
    "COMORBIDITY_SUBSTANCE_USE_RATE_EU = 0.30\n",
    "\n",
    "# --- Drug Profiles ---\n",
    "DRUG_PROFILES = {\n",
    "    'Placebo': {\n",
    "        'effect_per_unit': 0, 'unit_mg': 0, 'effect_std_bonus': 0, 'neg_symptom_bonus': 0,\n",
    "        'cog_effect_per_unit': 0, 'weight_gain_per_unit': 0, 'bmi_change_per_unit': 0,\n",
    "        'glucose_change_per_unit': 0, 'ldl_change_per_unit': 0, 'eps_effect_per_unit': 0,\n",
    "        'ae_bonus_factor': 0,\n",
    "        'MOA': 'None', 'Drug_Class': 'Placebo', 'Sponsor': 'N/A', 'available_doses': [0] \n",
    "    },\n",
    "    'Drug_A': { # Moderate all-rounder\n",
    "        'effect_per_unit': -2.5, 'unit_mg': 10, 'effect_std_bonus': 1.0, 'neg_symptom_bonus': 0,\n",
    "        'cog_effect_per_unit': 0.02, 'weight_gain_per_unit': 0.7, 'bmi_change_per_unit': 0.25,\n",
    "        'glucose_change_per_unit': 2.0, 'ldl_change_per_unit': 3.0, 'eps_effect_per_unit': 0.05,\n",
    "        'ae_bonus_factor': 0.05, 'available_doses': [10, 20, 40],\n",
    "        'MOA': 'D2/5HT2A Antagonist', 'Drug_Class': 'SGA', 'Sponsor': 'PharmaCo A' \n",
    "    },\n",
    "    'Drug_B': { # Stronger, slight negative focus, some EPS\n",
    "        'effect_per_unit': -4.0, 'unit_mg': 5, 'effect_std_bonus': 1.5, 'neg_symptom_bonus': -1.0,\n",
    "        'cog_effect_per_unit': 0.08, 'weight_gain_per_unit': 0.2, 'bmi_change_per_unit': 0.08,\n",
    "        'glucose_change_per_unit': 0.5, 'ldl_change_per_unit': 0.8, 'eps_effect_per_unit': 0.25,\n",
    "        'ae_bonus_factor': 0.16, 'available_doses': [5, 10],\n",
    "        'MOA': 'Potent D2 Antagonist', 'Drug_Class': 'SGA', 'Sponsor': 'PharmaCo B' \n",
    "    },\n",
    "    'Drug_C': { # Negative Symptom Focus, Moderate Efficacy, Metabolic Risk\n",
    "        'effect_per_unit': -2.0, 'unit_mg': 20, 'effect_std_bonus': 1.2, 'neg_symptom_bonus': -1.5,\n",
    "        'cog_effect_per_unit': 0.03, 'weight_gain_per_unit': 1.0, 'bmi_change_per_unit': 0.35,\n",
    "        'glucose_change_per_unit': 2.5, 'ldl_change_per_unit': 4.0, 'eps_effect_per_unit': 0.08,\n",
    "        'ae_bonus_factor': 0.04, 'available_doses': [20, 40, 60],\n",
    "        'MOA': '5HT2A/D2 Antagonist', 'Drug_Class': 'SGA', 'Sponsor': 'PharmaCo A' \n",
    "    },\n",
    "    'Drug_D': { # High Efficacy (Positive?), High EPS Risk\n",
    "        'effect_per_unit': -5.0, 'unit_mg': 10, 'effect_std_bonus': 1.8, 'neg_symptom_bonus': 0.5,\n",
    "        'cog_effect_per_unit': 0.01, 'weight_gain_per_unit': 0.3, 'bmi_change_per_unit': 0.1,\n",
    "        'glucose_change_per_unit': 0.8, 'ldl_change_per_unit': 1.0, 'eps_effect_per_unit': 0.40,\n",
    "        'ae_bonus_factor': 0.10, 'available_doses': [5, 10, 15],\n",
    "        'MOA': 'Strong D2 Antagonist', 'Drug_Class': 'FGA-like SGA', 'Sponsor': 'PharmaCo C' \n",
    "    },\n",
    "    'Drug_E': { # Lower Efficacy, Good Tolerability, Cognitive Edge\n",
    "        'effect_per_unit': -1.5, 'unit_mg': 50, 'effect_std_bonus': 0.8, 'neg_symptom_bonus': -0.5,\n",
    "        'cog_effect_per_unit': 0.12, 'weight_gain_per_unit': 0.1, 'bmi_change_per_unit': 0.04,\n",
    "        'glucose_change_per_unit': 0.2, 'ldl_change_per_unit': 0.3, 'eps_effect_per_unit': 0.02,\n",
    "        'ae_bonus_factor': 0.01, 'available_doses': [50, 100, 150],\n",
    "        'MOA': 'Partial D2 Agonist', 'Drug_Class': 'TGA', 'Sponsor': 'PharmaCo B' \n",
    "    },\n",
    "    'Drug_F': { # Similar Efficacy to A, Less Metabolic, Higher AE count\n",
    "        'effect_per_unit': -2.6, 'unit_mg': 10, 'effect_std_bonus': 1.1, 'neg_symptom_bonus': -0.2,\n",
    "        'cog_effect_per_unit': 0.03, 'weight_gain_per_unit': 0.4, 'bmi_change_per_unit': 0.15,\n",
    "        'glucose_change_per_unit': 1.0, 'ldl_change_per_unit': 1.5,\n",
    "        'eps_effect_per_unit': 0.06,\n",
    "        'ae_bonus_factor': 0.08, 'available_doses': [10, 20, 30],\n",
    "        'MOA': 'D2/5HT2A/5HT1A', 'Drug_Class': 'SGA', 'Sponsor': 'PharmaCo C' \n",
    "    },\n",
    "    'Drug_G': { # Adjunctive candidate? Low PANSS effect, good Cog/Neg, Tolerable\n",
    "        'effect_per_unit': -1.0, 'unit_mg': 25, 'effect_std_bonus': 0.7, 'neg_symptom_bonus': -1.0,\n",
    "        'cog_effect_per_unit': 0.15,\n",
    "        'weight_gain_per_unit': 0.05, 'bmi_change_per_unit': 0.02,\n",
    "        'glucose_change_per_unit': 0.1, 'ldl_change_per_unit': 0.2,\n",
    "        'eps_effect_per_unit': 0.01,\n",
    "        'ae_bonus_factor': 0.02, 'available_doses': [25, 50, 75],\n",
    "        'MOA': 'Glycine Modulator', 'Drug_Class': 'Adjunctive', 'Sponsor': 'Academia Inc' \n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# --- Study Configurations ---\n",
    "STUDIES_CONFIG = [\n",
    "    {\"Study_ID\": \"ACUTE_EFF_001\", \"N_Patients\": 800, \"Drugs\": ['Placebo', 'Drug_A'], \"Doses\": {'Drug_A': [10, 20, 40]}, \"Percent_US\": 0.60, \"Duration_Weeks\": 12},\n",
    "    {\"Study_ID\": \"NEG_SYM_002\", \"N_Patients\": 750, \"Drugs\": ['Placebo', 'Drug_B', 'Drug_C'], \"Doses\": {'Drug_B': [5, 10], 'Drug_C': [20, 40]}, \"Percent_US\": 0.30, \"Duration_Weeks\": 12},\n",
    "    {\"Study_ID\": \"COMPARE_003\", \"N_Patients\": 1200, \"Drugs\": ['Placebo', 'Drug_A', 'Drug_B'], \"Doses\": {'Drug_A': [20], 'Drug_B': [10]}, \"Percent_US\": 0.50, \"Duration_Weeks\": 10},\n",
    "    {\"Study_ID\": \"DOSE_FIND_A_004\", \"N_Patients\": 600, \"Drugs\": ['Placebo', 'Drug_A'], \"Doses\": {'Drug_A': [5, 10, 20, 40]}, \"Percent_US\": 0.80, \"Duration_Weeks\": 8},\n",
    "    {\"Study_ID\": \"NEG_FOCUS_005\", \"N_Patients\": 900, \"Drugs\": ['Placebo', 'Drug_C', 'Drug_E'], \"Doses\": {'Drug_C': [40], 'Drug_E': [100, 150]}, \"Percent_US\": 0.40, \"Duration_Weeks\": 16},\n",
    "    {\"Study_ID\": \"HIGH_EPS_COMP_006\", \"N_Patients\": 1000, \"Drugs\": ['Placebo', 'Drug_B', 'Drug_D'], \"Doses\": {'Drug_B': [10], 'Drug_D': [10, 15]}, \"Percent_US\": 0.55, \"Duration_Weeks\": 10},\n",
    "    {\"Study_ID\": \"LOW_DOSE_E_007\", \"N_Patients\": 700, \"Drugs\": ['Placebo', 'Drug_E'], \"Doses\": {'Drug_E': [50, 100]}, \"Percent_US\": 0.70, \"Duration_Weeks\": 12},\n",
    "    {\"Study_ID\": \"EU_DRUG_D_008\", \"N_Patients\": 850, \"Drugs\": ['Placebo', 'Drug_D'], \"Doses\": {'Drug_D': [5, 10]}, \"Percent_US\": 0.15, \"Duration_Weeks\": 8},\n",
    "    {\"Study_ID\": \"US_DRUG_C_009\", \"N_Patients\": 950, \"Drugs\": ['Placebo', 'Drug_C'], \"Doses\": {'Drug_C': [20, 40, 60]}, \"Percent_US\": 0.85, \"Duration_Weeks\": 12},\n",
    "    {\"Study_ID\": \"MIX_ALL_010\", \"N_Patients\": 1500, \"Drugs\": ['Placebo', 'Drug_A', 'Drug_C', 'Drug_E'], \"Doses\": {'Drug_A': [20], 'Drug_C': [40], 'Drug_E': [100]}, \"Percent_US\": 0.50, \"Duration_Weeks\": 12},\n",
    "    {\"Study_ID\": \"ACUTE_COMP_011\", \"N_Patients\": 1100, \"Drugs\": ['Placebo', 'Drug_A', 'Drug_D'], \"Doses\": {'Drug_A': [40], 'Drug_D': [10]}, \"Percent_US\": 0.65, \"Duration_Weeks\": 8},\n",
    "    {\"Study_ID\": \"MAINTENANCE_012\", \"N_Patients\": 1300, \"Drugs\": ['Placebo', 'Drug_A', 'Drug_E'], \"Doses\": {'Drug_A': [20], 'Drug_E': [100]}, \"Percent_US\": 0.45, \"Duration_Weeks\": 24},\n",
    "    {\"Study_ID\": \"EU_MIX_013\", \"N_Patients\": 1000, \"Drugs\": ['Placebo', 'Drug_B', 'Drug_C'], \"Doses\": {'Drug_B': [5], 'Drug_C': [20, 40]}, \"Percent_US\": 0.25, \"Duration_Weeks\": 10},\n",
    "    {\"Study_ID\": \"US_MIX_014\", \"N_Patients\": 1200, \"Drugs\": ['Placebo', 'Drug_A', 'Drug_E'], \"Doses\": {'Drug_A': [10, 20], 'Drug_E': [50, 100]}, \"Percent_US\": 0.75, \"Duration_Weeks\": 12},\n",
    "    {\"Study_ID\": \"DOSE_FIND_D_015\", \"N_Patients\": 800, \"Drugs\": ['Placebo', 'Drug_D'], \"Doses\": {'Drug_D': [5, 10, 15, 20]}, \"Percent_US\": 0.50, \"Duration_Weeks\": 6},\n",
    "    {\"Study_ID\": \"DOSE_FIND_C_016\", \"N_Patients\": 900, \"Drugs\": ['Placebo', 'Drug_C'], \"Doses\": {'Drug_C': [20, 40, 60, 80]}, \"Percent_US\": 0.60, \"Duration_Weeks\": 10},\n",
    "    {\"Study_ID\": \"BALANCED_AE_017\", \"N_Patients\": 1100, \"Drugs\": ['Placebo', 'Drug_A', 'Drug_E'], \"Doses\": {'Drug_A': [20], 'Drug_E': [100]}, \"Percent_US\": 0.50, \"Duration_Weeks\": 12},\n",
    "    {\"Study_ID\": \"HEAD_TO_HEAD_018\", \"N_Patients\": 1400, \"Drugs\": ['Drug_A', 'Drug_D'], \"Doses\": {'Drug_A': [40], 'Drug_D': [10]}, \"Percent_US\": 0.50, \"Duration_Weeks\": 10}, # No Placebo\n",
    "    {\"Study_ID\": \"ADD_ON_NEG_019\", \"N_Patients\": 850, \"Drugs\": ['Placebo', 'Drug_C'], \"Doses\": {'Drug_C': [20, 40]}, \"Percent_US\": 0.35, \"Duration_Weeks\": 16}, # Assumes add-on context for effect size interpretation\n",
    "    {\"Study_ID\": \"LONG_TERM_TOL_020\", \"N_Patients\": 1000, \"Drugs\": ['Placebo', 'Drug_E'], \"Doses\": {'Drug_E': [100, 150]}, \"Percent_US\": 0.60, \"Duration_Weeks\": 26},\n",
    "    {\"Study_ID\": \"DOSE_FIND_E_021\", \"N_Patients\": 750, \"Drugs\": ['Placebo', 'Drug_E'], \"Doses\": {'Drug_E': [50, 100, 150, 200]}, \"Percent_US\": 0.45, \"Duration_Weeks\": 10},\n",
    "    {\"Study_ID\": \"COMPARE_F_A_022\", \"N_Patients\": 1300, \"Drugs\": ['Placebo', 'Drug_A', 'Drug_F'], \"Doses\": {'Drug_A': [20], 'Drug_F': [20]}, \"Percent_US\": 0.50, \"Duration_Weeks\": 12}, # Compare A and new F\n",
    "    {\"Study_ID\": \"US_DRUG_F_023\", \"N_Patients\": 900, \"Drugs\": ['Placebo', 'Drug_F'], \"Doses\": {'Drug_F': [10, 20, 30]}, \"Percent_US\": 0.90, \"Duration_Weeks\": 8},\n",
    "    {\"Study_ID\": \"EU_DRUG_F_024\", \"N_Patients\": 800, \"Drugs\": ['Placebo', 'Drug_F'], \"Doses\": {'Drug_F': [10, 20]}, \"Percent_US\": 0.10, \"Duration_Weeks\": 10},\n",
    "    {\"Study_ID\": \"ADJUNCT_COG_G_025\", \"N_Patients\": 950, \"Drugs\": ['Placebo', 'Drug_G'], \"Doses\": {'Drug_G': [50, 75]}, \"Percent_US\": 0.55, \"Duration_Weeks\": 16}, # Test G as adjunct (assume context)\n",
    "    {\"Study_ID\": \"COMPARE_G_E_026\", \"N_Patients\": 1100, \"Drugs\": ['Placebo', 'Drug_E', 'Drug_G'], \"Doses\": {'Drug_E': [100], 'Drug_G': [50]}, \"Percent_US\": 0.40, \"Duration_Weeks\": 12}, # Compare low AE / Cog drugs\n",
    "    {\"Study_ID\": \"LARGE_GLOBAL_027\", \"N_Patients\": 2000, \"Drugs\": ['Placebo', 'Drug_A', 'Drug_F'], \"Doses\": {'Drug_A': [20], 'Drug_F': [20]}, \"Percent_US\": 0.50, \"Duration_Weeks\": 12}, # Very large Phase 3\n",
    "    {\"Study_ID\": \"LONG_MAINT_028\", \"N_Patients\": 1500, \"Drugs\": ['Placebo', 'Drug_A', 'Drug_F'], \"Doses\": {'Drug_A': [20], 'Drug_F': [10]}, \"Percent_US\": 0.60, \"Duration_Weeks\": 52}, # 1 year study\n",
    "    {\"Study_ID\": \"SHORT_ACUTE_D_029\", \"N_Patients\": 650, \"Drugs\": ['Placebo', 'Drug_D'], \"Doses\": {'Drug_D': [10]}, \"Percent_US\": 0.70, \"Duration_Weeks\": 6},\n",
    "    {\"Study_ID\": \"EU_COG_FOCUS_030\", \"N_Patients\": 850, \"Drugs\": ['Placebo', 'Drug_E', 'Drug_G'], \"Doses\": {'Drug_E': [150], 'Drug_G': [75]}, \"Percent_US\": 0.20, \"Duration_Weeks\": 16},\n",
    "    {\"Study_ID\": \"MIX_BCF_031\", \"N_Patients\": 1250, \"Drugs\": ['Placebo', 'Drug_B', 'Drug_C', 'Drug_F'], \"Doses\": {'Drug_B': [10], 'Drug_C': [40], 'Drug_F': [20]}, \"Percent_US\": 0.50, \"Duration_Weeks\": 10},\n",
    "    {\"Study_ID\": \"HEAD_TO_HEAD_BF_032\", \"N_Patients\": 1000, \"Drugs\": ['Drug_B', 'Drug_F'], \"Doses\": {'Drug_B': [10], 'Drug_F': [30]}, \"Percent_US\": 0.60, \"Duration_Weeks\": 12}, # Active comparator\n",
    "    {\"Study_ID\": \"PHASE_II_G_033\", \"N_Patients\": 450, \"Drugs\": ['Placebo', 'Drug_G'], \"Doses\": {'Drug_G': [25, 50, 75]}, \"Percent_US\": 0.40, \"Duration_Weeks\": 8}, # Smaller Phase II size\n",
    "    {\"Study_ID\": \"PHASE_II_F_034\", \"N_Patients\": 500, \"Drugs\": ['Placebo', 'Drug_F'], \"Doses\": {'Drug_F': [5, 10, 20, 30]}, \"Percent_US\": 0.70, \"Duration_Weeks\": 8},\n",
    "    {\"Study_ID\": \"SWITCH_STUDY_AE_035\", \"N_Patients\": 900, \"Drugs\": ['Drug_A', 'Drug_E'], \"Doses\": {'Drug_A': [20], 'Drug_E': [100]}, \"Percent_US\": 0.50, \"Duration_Weeks\": 24}, # Simulating switch for tolerability (analyze baseline differently in reality)\n",
    "    {\"Study_ID\": \"GLOBAL_NEG_SYM_036\", \"N_Patients\": 1600, \"Drugs\": ['Placebo', 'Drug_C', 'Drug_G'], \"Doses\": {'Drug_C': [40], 'Drug_G': [50]}, \"Percent_US\": 0.48, \"Duration_Weeks\": 16},\n",
    "    {\"Study_ID\": \"US_ONLY_A_HIGH_037\", \"N_Patients\": 700, \"Drugs\": ['Placebo', 'Drug_A'], \"Doses\": {'Drug_A': [40, 60]}, \"Percent_US\": 1.00, \"Duration_Weeks\": 10}, # Higher doses of A, US only\n",
    "    {\"Study_ID\": \"EU_ONLY_B_LOW_038\", \"N_Patients\": 600, \"Drugs\": ['Placebo', 'Drug_B'], \"Doses\": {'Drug_B': [2.5, 5]}, \"Percent_US\": 0.00, \"Duration_Weeks\": 12}, # Lower doses of B, EU only\n",
    "    {\"Study_ID\": \"COMPARE_D_F_039\", \"N_Patients\": 1150, \"Drugs\": ['Placebo', 'Drug_D', 'Drug_F'], \"Doses\": {'Drug_D': [10], 'Drug_F': [20]}, \"Percent_US\": 0.50, \"Duration_Weeks\": 8},\n",
    "    {\"Study_ID\": \"LONG_TERM_G_ADJ_040\", \"N_Patients\": 1000, \"Drugs\": ['Placebo', 'Drug_G'], \"Doses\": {'Drug_G': [50]}, \"Percent_US\": 0.55, \"Duration_Weeks\": 52}, # Long term adjunct G\n",
    "]\n",
    "\n",
    "\n",
    "# Total patients for progress tracking\n",
    "TOTAL_PATIENTS_TO_GENERATE = sum(study['N_Patients'] for study in STUDIES_CONFIG)\n",
    "\n",
    "\n",
    "# --- Baseline & Simulation Parameters ---\n",
    "AGE_MEAN, AGE_STD = 35, 8\n",
    "SEX_DIST = {'Male': 0.65, 'Female': 0.35}\n",
    "YEARS_DX_MEAN, YEARS_DX_STD = 7, 5\n",
    "AGE_ONSET_MEAN, AGE_ONSET_STD = 24, 6\n",
    "EDUCATION_DIST = {'High School/GED': 0.45, 'Some College/Associate': 0.35, 'Bachelor Degree+': 0.20}\n",
    "SES_DIST = {'Low': 0.40, 'Medium': 0.50, 'High': 0.10}\n",
    "BMI_MEAN, BMI_STD = 27, 4\n",
    "GLUCOSE_MEAN, GLUCOSE_STD = 95, 15\n",
    "LDL_MEAN, LDL_STD = 110, 25\n",
    "CV_RISK_RATE = 0.20\n",
    "\n",
    "# Baseline Clinical\n",
    "PANSS_TOTAL_MEAN, PANSS_TOTAL_STD = 90, 12\n",
    "PANSS_NEG_MEAN_RATIO = 0.25\n",
    "PANSS_POS_MEAN_RATIO = 0.23\n",
    "BPRS_BASELINE_MEAN_RATIO = 0.5\n",
    "BPRS_BASELINE_STD_RATIO = 0.05\n",
    "SANS_BASELINE_MEAN_RATIO = 1.5\n",
    "SANS_BASELINE_STD_RATIO = 0.2\n",
    "CGI_S_BASELINE_DIST = {4: 0.2, 5: 0.5, 6: 0.3}\n",
    "GAF_BASELINE_MEAN, GAF_BASELINE_STD = 45, 8\n",
    "QOL_BASELINE_MEAN, QOL_BASELINE_STD = 50, 10\n",
    "COG_BASELINE_MEAN, COG_BASELINE_STD = -1.0, 0.7\n",
    "PREV_AP_MEAN, PREV_AP_STD = 2.5, 1.5\n",
    "\n",
    "# Outcome Simulation Parameters\n",
    "BPRS_CHANGE_FACTOR = 0.55\n",
    "SANS_CHANGE_FACTOR = 1.6\n",
    "CGI_CHANGE_SCALE_FACTOR = 0.1\n",
    "PLACEBO_GAF_CHANGE_MEAN, PLACEBO_GAF_CHANGE_STD = 1.5, 4\n",
    "PLACEBO_QOL_CHANGE_MEAN, PLACEBO_QOL_CHANGE_STD = 2, 5\n",
    "PLACEBO_COG_CHANGE_MEAN, PLACEBO_COG_CHANGE_STD = 0.05, 0.2\n",
    "PLACEBO_WEIGHT_GAIN_MEAN, PLACEBO_WEIGHT_GAIN_STD = 0.5, 1.0\n",
    "PLACEBO_BMI_CHANGE_MEAN, PLACEBO_BMI_CHANGE_STD = 0.2, 0.4\n",
    "PLACEBO_GLUCOSE_CHANGE_MEAN, PLACEBO_GLUCOSE_CHANGE_STD = 1, 5\n",
    "PLACEBO_LDL_CHANGE_MEAN, PLACEBO_LDL_CHANGE_STD = 1, 8\n",
    "PLACEBO_EPS_SAS_CHANGE_MEAN, PLACEBO_EPS_SAS_CHANGE_STD = 0.1, 0.3\n",
    "PLACEBO_AE_COUNT_MEAN, PLACEBO_AE_COUNT_STD = 1.5, 1.0\n",
    "AE_DISCONTINUE_BASE_RATE = 0.03\n",
    "AE_DISCONTINUE_PER_AE = 0.02\n",
    "AE_DISCONTINUE_PER_KG_WG = 0.01\n",
    "AE_DISCONTINUE_PER_EPS_UNIT = 0.03\n",
    "\n",
    "\n",
    "# --- Data Generation Function ---\n",
    "def generate_patient_record(patient_id_counter, study_config):\n",
    "    patient = {}\n",
    "    patient['Patient_ID'] = f\"P{patient_id_counter}\"\n",
    "    patient['Study_ID'] = study_config[\"Study_ID\"]\n",
    "\n",
    "    percent_us = study_config[\"Percent_US\"]\n",
    "    is_us = random.random() < percent_us\n",
    "    patient['Region'] = 'US' if is_us else 'EU'\n",
    "\n",
    "    if is_us:\n",
    "        patient['Ethnicity'] = random.choices(ETHNICITY_CATEGORIES, weights=ETHNICITY_WEIGHTS_US, k=1)[0]\n",
    "        patient['Site_Type'] = random.choices(\n",
    "            ['Academic Hospital', 'Private Clinic', 'VA Hospital', 'Community Mental Health Center'],\n",
    "            weights=[0.35, 0.40, 0.15, 0.10], k=1)[0]\n",
    "        age_mod, years_dx_mod, panss_total_mod = 0, -0.5, 1\n",
    "        ses_weights = [0.35, 0.55, 0.10]\n",
    "        substance_use_rate = COMORBIDITY_SUBSTANCE_USE_RATE_US\n",
    "    else: # EU\n",
    "        patient['Ethnicity'] = random.choices(ETHNICITY_CATEGORIES, weights=ETHNICITY_WEIGHTS_EU, k=1)[0]\n",
    "        patient['Site_Type'] = random.choices(\n",
    "            ['University Hospital', 'National Health Service Clinic', 'Private Practice'],\n",
    "            weights=[0.55, 0.35, 0.10], k=1)[0]\n",
    "        age_mod, years_dx_mod, panss_total_mod = 0, 0.5, -1\n",
    "        ses_weights = [0.45, 0.48, 0.07]\n",
    "        substance_use_rate = COMORBIDITY_SUBSTANCE_USE_RATE_EU\n",
    "\n",
    "    # --- Demographics & Baseline ---\n",
    "    patient['Age'] = max(18, int(np.random.normal(AGE_MEAN + age_mod, AGE_STD)))\n",
    "    patient['Sex'] = random.choices(list(SEX_DIST.keys()), weights=list(SEX_DIST.values()), k=1)[0]\n",
    "    patient['Age_at_Onset'] = max(14, int(np.random.normal(min(patient['Age']-1, AGE_ONSET_MEAN), AGE_ONSET_STD)))\n",
    "    years_since_onset = patient['Age'] - patient['Age_at_Onset']\n",
    "    patient['Years_Since_Dx'] = max(0.1, round(np.random.normal(years_since_onset + years_dx_mod, YEARS_DX_STD / 2), 1))\n",
    "    patient['Education_Level'] = random.choices(list(EDUCATION_DIST.keys()), weights=list(EDUCATION_DIST.values()), k=1)[0]\n",
    "    patient['Socioeconomic_Status'] = random.choices(list(SES_DIST.keys()), weights=ses_weights, k=1)[0]\n",
    "    patient['Baseline_BMI'] = round(max(16, np.random.normal(BMI_MEAN, BMI_STD)), 1)\n",
    "    patient['Baseline_Glucose_mgdL'] = int(max(60, np.random.normal(GLUCOSE_MEAN, GLUCOSE_STD)))\n",
    "    patient['Baseline_LDL_mgdL'] = int(max(50, np.random.normal(LDL_MEAN, LDL_STD)))\n",
    "    patient['Cardiovascular_Risk_Factor'] = 1 if random.random() < CV_RISK_RATE else 0\n",
    "    patient['Comorbidity_Substance_Use'] = 1 if random.random() < substance_use_rate else 0\n",
    "\n",
    "    # --- Baseline Clinical ---\n",
    "    patient['Baseline_PANSS_Total'] = int(np.random.normal(PANSS_TOTAL_MEAN + panss_total_mod, PANSS_TOTAL_STD))\n",
    "    base_neg_prop = np.random.normal(PANSS_NEG_MEAN_RATIO, 0.06)\n",
    "    base_pos_prop = np.random.normal(PANSS_POS_MEAN_RATIO, 0.06)\n",
    "    base_gen_prop = max(0, 1.0 - base_neg_prop - base_pos_prop)\n",
    "    norm_factor = base_neg_prop + base_pos_prop + base_gen_prop\n",
    "    base_neg_prop /= norm_factor\n",
    "    base_pos_prop /= norm_factor\n",
    "    base_gen_prop /= norm_factor\n",
    "    patient['Baseline_PANSS_Negative'] = max(7, int(patient['Baseline_PANSS_Total'] * base_neg_prop))\n",
    "    patient['Baseline_PANSS_Positive'] = max(7, int(patient['Baseline_PANSS_Total'] * base_pos_prop))\n",
    "    patient['Baseline_PANSS_General'] = max(16, int(patient['Baseline_PANSS_Total'] * base_gen_prop))\n",
    "    patient['Baseline_PANSS_Total'] = patient['Baseline_PANSS_Negative'] + patient['Baseline_PANSS_Positive'] + patient['Baseline_PANSS_General']\n",
    "\n",
    "    patient['Baseline_BPRS'] = int(patient['Baseline_PANSS_Total'] * np.random.normal(BPRS_BASELINE_MEAN_RATIO, BPRS_BASELINE_STD_RATIO))\n",
    "    patient['Baseline_SANS'] = int(patient['Baseline_PANSS_Negative'] * np.random.normal(SANS_BASELINE_MEAN_RATIO, SANS_BASELINE_STD_RATIO))\n",
    "    patient['Baseline_CGI_S'] = random.choices(list(CGI_S_BASELINE_DIST.keys()), weights=list(CGI_S_BASELINE_DIST.values()), k=1)[0]\n",
    "    patient['Baseline_GAF'] = int(max(20, min(70, np.random.normal(GAF_BASELINE_MEAN, GAF_BASELINE_STD))))\n",
    "    patient['Baseline_QoL'] = int(max(10, min(90, np.random.normal(QOL_BASELINE_MEAN, QOL_BASELINE_STD))))\n",
    "    patient['Baseline_Cognitive_Score'] = round(np.random.normal(COG_BASELINE_MEAN, COG_BASELINE_STD), 2)\n",
    "    patient['Previous_Antipsychotics_Count'] = max(0, int(np.random.normal(PREV_AP_MEAN, PREV_AP_STD)))\n",
    "\n",
    "    # --- Treatment Assignment based on Study Config ---\n",
    "    available_drugs_in_study = study_config[\"Drugs\"]\n",
    "    if 'Placebo' not in available_drugs_in_study and len(available_drugs_in_study) > 0 :\n",
    "         patient['Drug'] = random.choice(available_drugs_in_study)\n",
    "    elif 'Placebo' in available_drugs_in_study:\n",
    "         patient['Drug'] = random.choice(available_drugs_in_study)\n",
    "    else:\n",
    "         patient['Drug'] = 'Placebo' # Default to placebo if error\n",
    "\n",
    "    drug_profile = DRUG_PROFILES[patient['Drug']] \n",
    "\n",
    "    # --- V3 Add: Get Drug Info ---\n",
    "    patient['Drug_MOA'] = drug_profile['MOA']\n",
    "    patient['Drug_Class'] = drug_profile['Drug_Class']\n",
    "    patient['Drug_Sponsor'] = drug_profile['Sponsor']\n",
    "\n",
    "    # --- Assign Dose ---\n",
    "    if patient['Drug'] == 'Placebo':\n",
    "        patient['Dose_mg_day'] = 0\n",
    "    else:\n",
    "        available_doses = study_config[\"Doses\"].get(patient['Drug'], drug_profile['available_doses']) # Fallback to profile doses\n",
    "        if not isinstance(available_doses, list) or len(available_doses) == 0:\n",
    "             available_doses = drug_profile['available_doses'] # Ensure fallback\n",
    "        patient['Dose_mg_day'] = random.choice(available_doses)\n",
    "\n",
    "    patient['Treatment_Duration_Weeks'] = study_config[\"Duration_Weeks\"]\n",
    "\n",
    "    # --- Simulate Outcomes ---\n",
    "    dose = patient['Dose_mg_day']\n",
    "    unit_mg = drug_profile['unit_mg']\n",
    "    dose_units = (dose / unit_mg) if unit_mg > 0 else 0\n",
    "\n",
    "    # Base Placebo Effects\n",
    "    placebo_effect_mean = PLACEBO_PANSS_CHANGE_MEAN_US if is_us else PLACEBO_PANSS_CHANGE_MEAN_EU\n",
    "    placebo_effect_std = PLACEBO_PANSS_CHANGE_STD_US if is_us else PLACEBO_PANSS_CHANGE_STD_EU\n",
    "    individual_responder_factor = np.random.normal(1.0, 0.3) # Individual variability\n",
    "\n",
    "    base_panss_change = np.random.normal(placebo_effect_mean, placebo_effect_std) * individual_responder_factor\n",
    "    # Distribute placebo effect across subscales proportionally\n",
    "    neg_prop = np.random.normal(PANSS_NEG_MEAN_RATIO, 0.08)\n",
    "    pos_prop = np.random.normal(PANSS_POS_MEAN_RATIO, 0.08)\n",
    "    gen_prop = max(0, 1.0 - neg_prop - pos_prop)\n",
    "    norm_f = neg_prop+pos_prop+gen_prop\n",
    "    neg_prop /= norm_f ; pos_prop /= norm_f ; gen_prop /= norm_f\n",
    "    base_panss_neg_change = base_panss_change * neg_prop\n",
    "    base_panss_pos_change = base_panss_change * pos_prop\n",
    "    base_panss_gen_change = base_panss_change * gen_prop\n",
    "\n",
    "    # Placebo effects for other outcomes\n",
    "    base_gaf_change = np.random.normal(PLACEBO_GAF_CHANGE_MEAN, PLACEBO_GAF_CHANGE_STD) * individual_responder_factor\n",
    "    base_qol_change = np.random.normal(PLACEBO_QOL_CHANGE_MEAN, PLACEBO_QOL_CHANGE_STD) * individual_responder_factor\n",
    "    base_cog_change = np.random.normal(PLACEBO_COG_CHANGE_MEAN, PLACEBO_COG_CHANGE_STD) * max(0.5, individual_responder_factor) # Less variable placebo cog effect\n",
    "    base_weight_change = np.random.normal(PLACEBO_WEIGHT_GAIN_MEAN, PLACEBO_WEIGHT_GAIN_STD)\n",
    "    base_bmi_change = np.random.normal(PLACEBO_BMI_CHANGE_MEAN, PLACEBO_BMI_CHANGE_STD)\n",
    "    base_glucose_change = np.random.normal(PLACEBO_GLUCOSE_CHANGE_MEAN, PLACEBO_GLUCOSE_CHANGE_STD)\n",
    "    base_ldl_change = np.random.normal(PLACEBO_LDL_CHANGE_MEAN, PLACEBO_LDL_CHANGE_STD)\n",
    "    base_eps_change = np.random.normal(PLACEBO_EPS_SAS_CHANGE_MEAN, PLACEBO_EPS_SAS_CHANGE_STD)\n",
    "    base_ae_count = max(0, np.random.normal(PLACEBO_AE_COUNT_MEAN, PLACEBO_AE_COUNT_STD))\n",
    "\n",
    "    # Add Drug Effects (if not Placebo)\n",
    "    drug_panss_effect, drug_panss_neg_effect, drug_panss_pos_effect, drug_panss_gen_effect = 0, 0, 0, 0\n",
    "    drug_gaf_effect, drug_qol_effect, drug_cog_effect = 0, 0, 0\n",
    "    drug_weight_effect, drug_bmi_effect, drug_glucose_effect, drug_ldl_effect = 0, 0, 0, 0\n",
    "    drug_eps_effect, drug_ae_bonus_calc = 0, 0\n",
    "\n",
    "    if patient['Drug'] != 'Placebo':\n",
    "        effect_mean = drug_profile['effect_per_unit'] * dose_units\n",
    "        effect_std = drug_profile['effect_std_bonus'] * math.sqrt(max(0.1, dose_units)) # Std scales with sqrt(dose units)\n",
    "        drug_panss_effect = np.random.normal(effect_mean, effect_std)\n",
    "\n",
    "        # Distribute drug effect across subscales, considering negative symptom\n",
    "        neg_bonus = drug_profile['neg_symptom_bonus'] * dose_units\n",
    "        remaining_effect = drug_panss_effect - neg_bonus\n",
    "        # Use baseline PANSS ratios to distribute remaining effect proportionally\n",
    "        total_ratio = PANSS_NEG_MEAN_RATIO + PANSS_POS_MEAN_RATIO + (1-PANSS_NEG_MEAN_RATIO-PANSS_POS_MEAN_RATIO)\n",
    "        neg_prop_drug = PANSS_NEG_MEAN_RATIO / total_ratio\n",
    "        pos_prop_drug = PANSS_POS_MEAN_RATIO / total_ratio\n",
    "        gen_prop_drug = (1 - PANSS_NEG_MEAN_RATIO - PANSS_POS_MEAN_RATIO) / total_ratio\n",
    "\n",
    "        drug_panss_neg_effect = remaining_effect * neg_prop_drug * np.random.normal(1, 0.1) + neg_bonus\n",
    "        drug_panss_pos_effect = remaining_effect * pos_prop_drug * np.random.normal(1, 0.1)\n",
    "        drug_panss_gen_effect = remaining_effect * gen_prop_drug * np.random.normal(1, 0.1)\n",
    "\n",
    "        # Other effects (simple linear scaling with dose units + noise)\n",
    "        drug_gaf_effect = np.random.normal(1.0 * dose_units, 1.0 * math.sqrt(max(0.1, dose_units))) # GAF effect\n",
    "        drug_qol_effect = np.random.normal(1.2 * dose_units, 1.2 * math.sqrt(max(0.1, dose_units))) # QoL effect\n",
    "        drug_cog_effect = np.random.normal(drug_profile['cog_effect_per_unit'] * dose_units, 0.05 * math.sqrt(max(0.1, dose_units))) # Cog effect\n",
    "\n",
    "        # Safety effects\n",
    "        drug_weight_effect = np.random.normal(drug_profile['weight_gain_per_unit'] * dose_units, 0.5 * math.sqrt(max(0.1, dose_units)))\n",
    "        drug_bmi_effect = np.random.normal(drug_profile['bmi_change_per_unit'] * dose_units, 0.1 * math.sqrt(max(0.1, dose_units)))\n",
    "        drug_glucose_effect = np.random.normal(drug_profile['glucose_change_per_unit'] * dose_units, 1.0 * math.sqrt(max(0.1, dose_units)))\n",
    "        drug_ldl_effect = np.random.normal(drug_profile['ldl_change_per_unit'] * dose_units, 1.5 * math.sqrt(max(0.1, dose_units)))\n",
    "        drug_eps_effect = np.random.normal(drug_profile['eps_effect_per_unit'] * dose_units, 0.1 * math.sqrt(max(0.1, dose_units)))\n",
    "        drug_ae_bonus_calc = drug_profile['ae_bonus_factor'] * dose_units * np.random.normal(1.0, 0.2) # AE count\n",
    "\n",
    "    # Individual responder factor to drug effects\n",
    "    drug_panss_effect *= individual_responder_factor\n",
    "    drug_panss_neg_effect *= individual_responder_factor\n",
    "    drug_panss_pos_effect *= individual_responder_factor\n",
    "    drug_panss_gen_effect *= individual_responder_factor\n",
    "    drug_gaf_effect *= individual_responder_factor\n",
    "    drug_qol_effect *= individual_responder_factor\n",
    "    drug_cog_effect *= max(0.5, individual_responder_factor) # Apply cognitive effect\n",
    "\n",
    "    # --- Final Outcomes ---\n",
    "    # Combine Placebo + Drug effects for PANSS changes\n",
    "    patient['Change_PANSS_Total'] = round(base_panss_change + drug_panss_effect, 2)\n",
    "    patient['Endpoint_PANSS_Total'] = max(30, int(patient['Baseline_PANSS_Total'] + patient['Change_PANSS_Total']))\n",
    "    patient['Change_PANSS_Negative'] = round(base_panss_neg_change + drug_panss_neg_effect, 2)\n",
    "    patient['Endpoint_PANSS_Negative'] = max(7, int(patient['Baseline_PANSS_Negative'] + patient['Change_PANSS_Negative']))\n",
    "    patient['Change_PANSS_Positive'] = round(base_panss_pos_change + drug_panss_pos_effect, 2)\n",
    "    patient['Endpoint_PANSS_Positive'] = max(7, int(patient['Baseline_PANSS_Positive'] + patient['Change_PANSS_Positive']))\n",
    "    patient['Change_PANSS_General'] = round(base_panss_gen_change + drug_panss_gen_effect, 2)\n",
    "    patient['Endpoint_PANSS_General'] = max(16, int(patient['Baseline_PANSS_General'] + patient['Change_PANSS_General']))\n",
    "    patient['Endpoint_PANSS_Total'] = patient['Endpoint_PANSS_Negative'] + patient['Endpoint_PANSS_Positive'] + patient['Endpoint_PANSS_General']\n",
    "    patient['Change_PANSS_Total'] = patient['Endpoint_PANSS_Total'] - patient['Baseline_PANSS_Total']\n",
    "\n",
    "    # Other efficacy endpoints\n",
    "    patient['Change_BPRS'] = round(patient['Change_PANSS_Total'] * np.random.normal(BPRS_CHANGE_FACTOR, 0.1), 1)\n",
    "    patient['Change_SANS'] = round(patient['Change_PANSS_Negative'] * np.random.normal(SANS_CHANGE_FACTOR, 0.2), 1)\n",
    "    patient['Change_CGI_S'] = round(patient['Change_PANSS_Total'] * CGI_CHANGE_SCALE_FACTOR * np.random.normal(1, 0.1), 1)\n",
    "    patient['Endpoint_CGI_S'] = round(max(1, min(7, patient['Baseline_CGI_S'] + patient['Change_CGI_S'])), 1)\n",
    "\n",
    "    # Estimate CGI-I based on PANSS change bins + noise\n",
    "    panss_change = patient['Change_PANSS_Total']\n",
    "    if panss_change <= -25: cgi_i_base = 1\n",
    "    elif panss_change <= -18: cgi_i_base = 2\n",
    "    elif panss_change <= -8: cgi_i_base = 3\n",
    "    elif panss_change < 5: cgi_i_base = 4\n",
    "    elif panss_change < 15: cgi_i_base = 5\n",
    "    elif panss_change < 25: cgi_i_base = 6\n",
    "    else: cgi_i_base = 7\n",
    "    patient['Endpoint_CGI_I'] = max(1, min(7, cgi_i_base + random.choice([-1, 0, 0, 1]))) # Add some noise\n",
    "\n",
    "    patient['Change_GAF'] = round(base_gaf_change + drug_gaf_effect, 1)\n",
    "    patient['Endpoint_GAF'] = int(max(1, min(100, patient['Baseline_GAF'] + patient['Change_GAF'])))\n",
    "    patient['Change_QoL'] = round(base_qol_change + drug_qol_effect, 1)\n",
    "    patient['Endpoint_QoL'] = int(max(0, min(100, patient['Baseline_QoL'] + patient['Change_QoL'])))\n",
    "    patient['Change_Cognitive_Score'] = round(base_cog_change + drug_cog_effect, 2)\n",
    "    patient['Endpoint_Cognitive_Score'] = round(patient['Baseline_Cognitive_Score'] + patient['Change_Cognitive_Score'], 2)\n",
    "\n",
    "    # Safety outcomes\n",
    "    patient['Weight_Gain_kg'] = round(max(-5, base_weight_change + drug_weight_effect), 1) # Allow slight weight loss\n",
    "    patient['Change_BMI'] = round(base_bmi_change + drug_bmi_effect, 1)\n",
    "    patient['Endpoint_BMI'] = round(max(15, patient['Baseline_BMI'] + patient['Change_BMI']), 1) # Ensure BMI >= 15\n",
    "    patient['Change_Glucose'] = int(base_glucose_change + drug_glucose_effect)\n",
    "    patient['Endpoint_Glucose_mgdL'] = max(50, patient['Baseline_Glucose_mgdL'] + patient['Change_Glucose']) # Ensure Glucose >= 50\n",
    "    patient['Change_LDL'] = int(base_ldl_change + drug_ldl_effect)\n",
    "    patient['Endpoint_LDL_mgdL'] = max(40, patient['Baseline_LDL_mgdL'] + patient['Change_LDL']) # Ensure LDL >= 40\n",
    "    patient['EPS_SAS_Change'] = round(max(0, base_eps_change + drug_eps_effect), 1) # SAS change >= 0\n",
    "    patient['AE_Count'] = int(max(0, base_ae_count + drug_ae_bonus_calc)) # AE count >= 0\n",
    "\n",
    "    # Discontinuation logic\n",
    "    discontinuation_prob = AE_DISCONTINUE_BASE_RATE + \\\n",
    "                           patient['AE_Count'] * AE_DISCONTINUE_PER_AE + \\\n",
    "                           max(0, patient['Weight_Gain_kg']) * AE_DISCONTINUE_PER_KG_WG + \\\n",
    "                           patient['EPS_SAS_Change'] * AE_DISCONTINUE_PER_EPS_UNIT\n",
    "    if patient['Change_PANSS_Total'] < -15 or patient['Change_GAF'] > 8: discontinuation_prob *= 0.4 \n",
    "    if patient['Endpoint_CGI_I'] >= 6: discontinuation_prob *= 1.5 \n",
    "    patient['Discontinued_Due_To_AE'] = 1 if random.random() < max(0, min(1, discontinuation_prob)) else 0\n",
    "\n",
    "    # Responder and Remission Status\n",
    "    percent_change = (patient['Change_PANSS_Total'] / patient['Baseline_PANSS_Total']) if patient['Baseline_PANSS_Total'] > 0 else 0\n",
    "    patient['Responder_Status'] = 1 if (percent_change <= -0.30 or patient['Endpoint_CGI_I'] <= 2) else 0 # >=30% reduction or CGI-I <= 2\n",
    "    # Remission criteria\n",
    "    is_low_symptoms = patient['Endpoint_PANSS_Total'] <= 40 \n",
    "    is_good_function = patient['Endpoint_GAF'] >= 61 \n",
    "    is_stable_cgis = patient['Endpoint_CGI_S'] <= 3 \n",
    "    # Simple remission definition for simulation purposes\n",
    "    patient['Remission_Status'] = 1 if (is_low_symptoms and is_good_function and is_stable_cgis) else 0\n",
    "\n",
    "    return patient\n",
    "\n",
    "\n",
    "# --- Main Generation Loop ---\n",
    "start_main_time = time.time()\n",
    "print(f\"\\nStarting V3 Enhanced Generation for {len(STUDIES_CONFIG)} studies...\")\n",
    "print(f\"Targeting ~{TOTAL_PATIENTS_TO_GENERATE:,} total patient records.\")\n",
    "print(f\"Including regional demographics and Drugs A-G with MOA/Class/Sponsor.\") \n",
    "print(f\"Start time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "all_patient_data = []\n",
    "patient_id_counter = 50000 \n",
    "\n",
    "total_generated = 0\n",
    "for study_conf in STUDIES_CONFIG:\n",
    "    study_start_time = time.time()\n",
    "    n_study = study_conf['N_Patients']\n",
    "    print(f\"  Generating {n_study} patients for Study: {study_conf['Study_ID']} ({study_conf['Percent_US']*100:.0f}% US)...\")\n",
    "    print_freq = max(100, n_study // 5)\n",
    "\n",
    "    for i in range(n_study):\n",
    "        new_patient = generate_patient_record(patient_id_counter, study_conf)\n",
    "        all_patient_data.append(new_patient)\n",
    "        patient_id_counter += 1\n",
    "    total_generated += n_study\n",
    "    study_end_time = time.time()\n",
    "\n",
    "\n",
    "# --- Create Final DataFrame ---\n",
    "df_multi_study_v3 = pd.DataFrame(all_patient_data)\n",
    "end_main_time = time.time()\n",
    "print(f\"\\nFinished generating all {len(df_multi_study_v3):,} patient records from {len(STUDIES_CONFIG)} studies.\")\n",
    "print(f\"Total generation time: {end_main_time - start_main_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "# --- Column Order ---\n",
    "baseline_cols = [\n",
    "    'Baseline_PANSS_Total', 'Baseline_PANSS_Positive', 'Baseline_PANSS_Negative', 'Baseline_PANSS_General',\n",
    "    'Baseline_BPRS', 'Baseline_SANS', 'Baseline_CGI_S', 'Baseline_GAF', 'Baseline_QoL', 'Baseline_Cognitive_Score',\n",
    "    'Baseline_BMI', 'Baseline_Glucose_mgdL', 'Baseline_LDL_mgdL'\n",
    "]\n",
    "demographic_cols = [\n",
    "    'Patient_ID', 'Study_ID', 'Region', 'Site_Type', 'Age', 'Sex', 'Ethnicity', 'Education_Level', 'Socioeconomic_Status',\n",
    "    'Age_at_Onset', 'Years_Since_Dx', 'Previous_Antipsychotics_Count', 'Comorbidity_Substance_Use', 'Cardiovascular_Risk_Factor'\n",
    "]\n",
    "# Drug Info\n",
    "drug_info_cols = ['Drug', 'Dose_mg_day', 'Drug_MOA', 'Drug_Class', 'Drug_Sponsor']\n",
    "treatment_cols = ['Treatment_Duration_Weeks'] # Moved duration here\n",
    "\n",
    "endpoint_efficacy_cols = [\n",
    "    'Endpoint_PANSS_Total', 'Endpoint_PANSS_Positive', 'Endpoint_PANSS_Negative', 'Endpoint_PANSS_General',\n",
    "    'Endpoint_CGI_S', 'Endpoint_CGI_I', 'Endpoint_GAF', 'Endpoint_QoL', 'Endpoint_Cognitive_Score',\n",
    "    'Responder_Status', 'Remission_Status'\n",
    "]\n",
    "change_efficacy_cols = [\n",
    "     'Change_PANSS_Total', 'Change_PANSS_Positive', 'Change_PANSS_Negative', 'Change_PANSS_General',\n",
    "     'Change_BPRS', 'Change_SANS', 'Change_CGI_S', 'Change_GAF', 'Change_QoL', 'Change_Cognitive_Score'\n",
    "]\n",
    "safety_cols = [\n",
    "    'Weight_Gain_kg', 'Change_BMI', 'Endpoint_BMI',\n",
    "    'Change_Glucose', 'Endpoint_Glucose_mgdL', 'Change_LDL', 'Endpoint_LDL_mgdL',\n",
    "    'EPS_SAS_Change', 'AE_Count', 'Discontinued_Due_To_AE'\n",
    "]\n",
    "# column order\n",
    "all_cols = demographic_cols + drug_info_cols + treatment_cols + baseline_cols + \\\n",
    "           endpoint_efficacy_cols + change_efficacy_cols + safety_cols\n",
    "\n",
    "present_cols = [col for col in all_cols if col in df_multi_study_v3.columns]\n",
    "df_multi_study_v3 = df_multi_study_v3[present_cols]\n",
    "\n",
    "\n",
    "# --- Show and Save ---\n",
    "print(\"\\n--- V3 Enhanced Multi-Study Synthetic Schizophrenia Clinical Trial Dataset ---\")\n",
    "print(f\"Generated {len(df_multi_study_v3):,} total patient records across {len(STUDIES_CONFIG)} studies.\")\n",
    "print(f\"Total Drugs Modeled (Active + Placebo): {len(DRUG_PROFILES)}\")\n",
    "print(f\"Dataset includes Drug MOA, Class, and Sponsor.\") # V3 Update\n",
    "\n",
    "print(\"\\nDataset Head (First 5 rows):\")\n",
    "print(df_multi_study_v3.head().to_markdown(index=False))\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "df_multi_study_v3.info()\n",
    "\n",
    "print(\"\\nValue Counts for Study ID (Top 10):\")\n",
    "print(df_multi_study_v3['Study_ID'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nDrugs Tested per Study (Sample):\")\n",
    "try:\n",
    "    print(df_multi_study_v3.groupby('Study_ID')['Drug'].value_counts().sample(15, random_state=1))\n",
    "except ValueError:\n",
    "     print(df_multi_study_v3.groupby('Study_ID')['Drug'].value_counts())\n",
    "\n",
    "\n",
    "print(\"\\nRegion Distribution per Study (Summary):\")\n",
    "print(df_multi_study_v3.groupby('Study_ID')['Region'].value_counts(normalize=True).unstack().fillna(0).agg(['mean', 'median', 'min', 'max']))\n",
    "\n",
    "print(\"\\nEthnicity Distribution by Region:\")\n",
    "print(df_multi_study_v3.groupby('Region')['Ethnicity'].value_counts(normalize=True).unstack().fillna(0))\n",
    "\n",
    "# Save to CSV\n",
    "output_filename_v3 = \"synthetic_schizophrenia_multi_study_v3.csv\"\n",
    "df_multi_study_v3.to_csv(output_filename_v3, index=False)\n",
    "print(f\"\\nV3 Dataset saved to {output_filename_v3}\")\n",
    "\n",
    "# Display completion time\n",
    "current_time_str = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(f\"\\nProcessing completed at: {current_time_str}\")\n",
    "\n",
    "\n",
    "# --- Data Visualization ---\n",
    "print(\"\\n--- Generating Data Visualizations ---\")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "df_to_plot = df_multi_study_v3\n",
    "\n",
    "if df_to_plot is not None:\n",
    "\n",
    "    # --- General Distributions ---\n",
    "\n",
    "    # 1. Distribution of Baseline PANSS Total\n",
    "    print(\"  Plotting: Baseline PANSS Total Distribution\")\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(data=df_to_plot, x='Baseline_PANSS_Total', kde=True, bins=30)\n",
    "        plt.title('Distribution of Baseline PANSS Total')\n",
    "        plt.xlabel('Baseline PANSS Total Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig('v3_hist_baseline_panss.png') \n",
    "        plt.show() \n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"  Error plotting Baseline PANSS histogram: {e}\")\n",
    "\n",
    "    # 2. Distribution of Change in PANSS Total\n",
    "    print(\"  Plotting: Change in PANSS Total Distribution (All Arms)\")\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(data=df_to_plot, x='Change_PANSS_Total', kde=True, bins=40)\n",
    "        plt.title('Distribution of Change in PANSS Total (All Arms)')\n",
    "        plt.xlabel('Change from Baseline PANSS Total Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig('v3_hist_change_panss.png') \n",
    "        plt.show() \n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"  Error plotting Change PANSS histogram: {e}\")\n",
    "\n",
    "    # 3. Drug Assignment Counts\n",
    "    print(\"  Plotting: Patient Counts per Drug Assignment\")\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        sns.countplot(data=df_to_plot, y='Drug', order = df_to_plot['Drug'].value_counts().index)\n",
    "        plt.title('Patient Counts per Drug Assignment')\n",
    "        plt.xlabel('Number of Patients')\n",
    "        plt.ylabel('Drug')\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig('v3_count_drug.png')\n",
    "        plt.show() \n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"  Error plotting Drug counts: {e}\")\n",
    "\n",
    "\n",
    "    # --- US vs EU Comparisons ---\n",
    "\n",
    "    # 4. Change in PANSS Total by Region (Overall)\n",
    "    print(\"  Plotting: Change in PANSS Total by Region (Boxplot)\")\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.boxplot(data=df_to_plot, x='Region', y='Change_PANSS_Total', order=['EU', 'US'])\n",
    "        plt.title('Change in PANSS Total by Region (All Arms)')\n",
    "        plt.xlabel('Region')\n",
    "        plt.ylabel('Change from Baseline PANSS Total Score')\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig('v3_box_change_panss_region.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"  Error plotting Change PANSS boxplot by region: {e}\")\n",
    "\n",
    "    # 5. Placebo Response: Change in PANSS Total by Region\n",
    "    print(\"  Plotting: Placebo Response Distribution by Region (KDE)\")\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        df_placebo = df_to_plot[df_to_plot['Drug'] == 'Placebo'].copy()\n",
    "        sns.kdeplot(data=df_placebo, x='Change_PANSS_Total', hue='Region', fill=True, common_norm=False, hue_order=['EU', 'US'])\n",
    "        mean_eu = df_placebo[df_placebo['Region']=='EU']['Change_PANSS_Total'].mean()\n",
    "        mean_us = df_placebo[df_placebo['Region']=='US']['Change_PANSS_Total'].mean()\n",
    "        plt.axvline(mean_eu, color=sns.color_palette()[0], linestyle='--', label=f'EU Mean: {mean_eu:.2f}')\n",
    "        plt.axvline(mean_us, color=sns.color_palette()[1], linestyle='--', label=f'US Mean: {mean_us:.2f}')\n",
    "        plt.title('Placebo Response Distribution (Change in PANSS Total) by Region')\n",
    "        plt.xlabel('Change from Baseline PANSS Total Score')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig('v3_kde_placebo_panss_region.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"  Error plotting Placebo PANSS KDE by region: {e}\")\n",
    "\n",
    "    # 6. Example Drug Response: Change in PANSS for Drug D by Region\n",
    "    print(\"  Plotting: Drug D vs Placebo Response by Region\")\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        df_drug_d = df_to_plot[df_to_plot['Drug'] == 'Drug_D'].copy()\n",
    "        df_placebo_d_comp = pd.concat([df_placebo, df_drug_d]) # Reuse placebo df from above\n",
    "        sns.boxplot(data=df_placebo_d_comp, x='Region', y='Change_PANSS_Total', hue='Drug', order=['EU', 'US'], hue_order=['Placebo', 'Drug_D'])\n",
    "        plt.title('Drug D vs Placebo Response (Change PANSS) by Region')\n",
    "        plt.xlabel('Region')\n",
    "        plt.ylabel('Change from Baseline PANSS Total Score')\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig('v3_box_drug_d_vs_placebo_region.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"  Error plotting Drug D vs Placebo boxplot: {e}\")\n",
    "\n",
    "\n",
    "    # 7. Baseline PANSS Distribution by Region\n",
    "    print(\"  Plotting: Baseline PANSS Total Distribution by Region (KDE)\")\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.kdeplot(data=df_to_plot, x='Baseline_PANSS_Total', hue='Region', fill=True, common_norm=False, hue_order=['EU', 'US'])\n",
    "        plt.title('Baseline PANSS Total Distribution by Region')\n",
    "        plt.xlabel('Baseline PANSS Total Score')\n",
    "        plt.ylabel('Density')\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig('v3_kde_baseline_panss_region.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"  Error plotting Baseline PANSS KDE by region: {e}\")\n",
    "\n",
    "\n",
    "    # 8. Ethnicity Distribution by Region\n",
    "    print(\"  Plotting: Ethnicity Distribution by Region (Bar)\")\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        ethnicity_props = df_to_plot.groupby('Region')['Ethnicity'].value_counts(normalize=True).unstack().fillna(0)\n",
    "        ethnicity_props.plot(kind='bar', stacked=True, figsize=(10,6)) \n",
    "        plt.title('Proportion of Ethnicity Categories by Region')\n",
    "        plt.xlabel('Region')\n",
    "        plt.ylabel('Proportion')\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.legend(title='Ethnicity', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "        # plt.savefig('v3_bar_ethnicity_region.png')\n",
    "        plt.show() \n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"  Error plotting Ethnicity bar chart: {e}\")\n",
    "\n",
    "    # 9. Substance Use Comorbidity Rate by Region\n",
    "    print(\"  Plotting: Substance Use Comorbidity Rate by Region (Bar)\")\n",
    "    try:\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        sns.barplot(data=df_to_plot, x='Region', y='Comorbidity_Substance_Use', order=['EU', 'US'], estimator=np.mean, errorbar=None) # Use np.mean, removed ci for clarity\n",
    "        plt.title('Substance Use Comorbidity Rate by Region')\n",
    "        plt.xlabel('Region')\n",
    "        plt.ylabel('Proportion with Comorbidity')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig('v3_bar_substance_use_region.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"  Error plotting Substance Use bar chart: {e}\")\n",
    "\n",
    "\n",
    "    # 10. Distribution of Drug Classes\n",
    "    print(\"  Plotting: Distribution of Drug Classes\")\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.countplot(data=df_to_plot, y='Drug_Class', order=df_to_plot['Drug_Class'].value_counts().index)\n",
    "        plt.title('Patient Counts per Drug Class')\n",
    "        plt.xlabel('Number of Patients')\n",
    "        plt.ylabel('Drug Class')\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig('v3_count_drug_class.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"  Error plotting Drug Class counts: {e}\")\n",
    "\n",
    "    # 11. Change in PANSS by Drug Class (Example)\n",
    "    print(\"  Plotting: Change in PANSS by Drug Class (Boxplot)\")\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.boxplot(data=df_to_plot[df_to_plot['Drug_Class'] != 'Placebo'], x='Change_PANSS_Total', y='Drug_Class') # Excluded Placebo\n",
    "        plt.title('Change in PANSS Total by Active Drug Class')\n",
    "        plt.xlabel('Change from Baseline PANSS Total Score')\n",
    "        plt.ylabel('Drug Class')\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig('v3_box_change_panss_drug_class.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"  Error plotting Change PANSS by Drug Class: {e}\")\n",
    "\n",
    "    # 12. Change in PANSS by MOA (Top MOAs)\n",
    "    print(\"  Plotting: Change in PANSS by Top MOAs (Boxplot)\")\n",
    "    try:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        # Select top N MOAs excluding 'None' (Placebo) for clarity\n",
    "        top_moas = df_to_plot[df_to_plot['Drug_MOA'] != 'None']['Drug_MOA'].value_counts().nlargest(5).index\n",
    "        df_top_moa = df_to_plot[df_to_plot['Drug_MOA'].isin(top_moas)]\n",
    "        sns.boxplot(data=df_top_moa, x='Change_PANSS_Total', y='Drug_MOA', order=top_moas)\n",
    "        plt.title('Change in PANSS Total by Top 5 MOAs (Excluding Placebo)')\n",
    "        plt.xlabel('Change from Baseline PANSS Total Score')\n",
    "        plt.ylabel('Mechanism of Action (MOA)')\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig('v3_box_change_panss_moa.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\" Error plotting Change PANSS by MOA: {e}\")\n",
    "\n",
    "\n",
    "    print(\"--- Visualization generation complete ---\")\n",
    "\n",
    "else:\n",
    "    print(\"Problem.\")\n",
    "\n",
    "\n",
    "# --- End of Script ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Cycle GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional CycleGAN\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, LayerNormalization, LeakyReLU, Add,\n",
    "    Embedding, Concatenate, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "# Hyperparameters & Config\n",
    "\n",
    "DATASET_FILENAME = \"synthetic_schizophrenia_multi_study_v3.csv\"\n",
    "CHECKPOINT_DIR = \"./tf_checkpoints/cyclegan_cond_aug\"\n",
    "SAVE_FREQ_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "TOTAL_EPOCHS = 50\n",
    "\n",
    "# Embedding dimensions\n",
    "STUDY_EMBED_DIM = 16\n",
    "DRUG_EMBED_DIM = 8\n",
    "# Noise dimension for one-to-many\n",
    "Z_DIM = 16\n",
    "\n",
    "# CycleGAN loss weights\n",
    "LAMBDA_CYCLE = 10.0\n",
    "LAMBDA_ID = 0.1\n",
    "\n",
    "# Features\n",
    "FEATURES_X = [\n",
    "    'Change_PANSS_Total', 'Change_PANSS_Positive', 'Change_PANSS_Negative',\n",
    "    'Change_GAF', 'Change_Cognitive_Score', 'Weight_Gain_kg',\n",
    "    'EPS_SAS_Change', 'AE_Count'\n",
    "]\n",
    "FEATURES_C_CONT = [\n",
    "    'Age', 'Baseline_PANSS_Total', 'Age_at_Onset', 'Years_Since_Dx',\n",
    "    'Previous_Antipsychotics_Count', 'Dose_mg_day', 'Treatment_Duration_Weeks'\n",
    "]\n",
    "FEATURES_C_CAT = ['Study_ID', 'Drug']\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "# Dense Residual Block\n",
    "def dense_residual_block(x, width):\n",
    "    shortcut = x\n",
    "    y = Dense(width)(x)\n",
    "    y = LayerNormalization()(y)\n",
    "    y = LeakyReLU(alpha=0.2)(y)\n",
    "    y = Dense(width)(y)\n",
    "    y = LayerNormalization()(y)\n",
    "    out = Add()([shortcut, y])\n",
    "    out = LeakyReLU(alpha=0.2)(out)\n",
    "    return out\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "# Generator with Learned Embeddings & Noise\n",
    "\n",
    "def build_generator_cGAN(\n",
    "    input_dim_x, cont_dim,\n",
    "    n_studies, n_drugs,\n",
    "    study_emb_dim, drug_emb_dim, z_dim,\n",
    "    num_residual=6, layer_width=128\n",
    "):\n",
    "    inp_x     = Input(shape=(input_dim_x,), name='gen_input_x')\n",
    "    inp_cont  = Input(shape=(cont_dim,), name='gen_input_cont')\n",
    "    inp_study = Input(shape=(), dtype='int32', name='gen_input_study')\n",
    "    inp_drug  = Input(shape=(), dtype='int32', name='gen_input_drug')\n",
    "    inp_z     = Input(shape=(z_dim,), name='gen_input_z')\n",
    "\n",
    "    # learned embeddings\n",
    "    stud_emb = Embedding(n_studies, study_emb_dim, name='study_emb')(inp_study)\n",
    "    stud_emb = Flatten()(stud_emb)\n",
    "    drug_emb = Embedding(n_drugs, drug_emb_dim, name='drug_emb')(inp_drug)\n",
    "    drug_emb = Flatten()(drug_emb)\n",
    "\n",
    "    x = Concatenate()([inp_x, inp_cont, stud_emb, drug_emb, inp_z])\n",
    "    x = Dense(layer_width)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    for _ in range(num_residual):\n",
    "        x = dense_residual_block(x, layer_width)\n",
    "    out = Dense(input_dim_x, name='gen_output')(x)  # linear\n",
    "    return Model(\n",
    "        inputs=[inp_x, inp_cont, inp_study, inp_drug, inp_z],\n",
    "        outputs=out,\n",
    "        name=f'Generator_R{num_residual}_W{layer_width}_Z{z_dim}'\n",
    "    )\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "# Discriminator with Learned Embeddings\n",
    "\n",
    "def build_discriminator_cGAN(\n",
    "    input_dim_x, cont_dim,\n",
    "    n_studies, n_drugs,\n",
    "    study_emb_dim, drug_emb_dim,\n",
    "    num_layers=4, start_width=128, name_suffix=\"\"\n",
    "):\n",
    "    inp_x     = Input(shape=(input_dim_x,), name=f'disc_input_x{name_suffix}')\n",
    "    inp_cont  = Input(shape=(cont_dim,), name=f'disc_input_cont{name_suffix}')\n",
    "    inp_study = Input(shape=(), dtype='int32', name=f'disc_input_study{name_suffix}')\n",
    "    inp_drug  = Input(shape=(), dtype='int32', name=f'disc_input_drug{name_suffix}')\n",
    "\n",
    "    stud_emb = Embedding(n_studies, study_emb_dim, name=f'study_emb{name_suffix}')(inp_study)\n",
    "    stud_emb = Flatten()(stud_emb)\n",
    "    drug_emb = Embedding(n_drugs, drug_emb_dim, name=f'drug_emb{name_suffix}')(inp_drug)\n",
    "    drug_emb = Flatten()(drug_emb)\n",
    "\n",
    "    x = Concatenate()([inp_x, inp_cont, stud_emb, drug_emb])\n",
    "    x = Dense(start_width)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    curr_w = start_width\n",
    "    for _ in range(num_layers - 2):\n",
    "        curr_w = max(32, curr_w // 2)\n",
    "        x = Dense(curr_w)(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "    out = Dense(1, name=f'disc_output{name_suffix}')(x)  # linear real/fake score\n",
    "    return Model(\n",
    "        inputs=[inp_x, inp_cont, inp_study, inp_drug],\n",
    "        outputs=out,\n",
    "        name=f'Discriminator_L{num_layers}_W{start_width}{name_suffix}'\n",
    "    )\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "# Load & Prepare Data\n",
    "\n",
    "if not os.path.exists(DATASET_FILENAME):\n",
    "    raise FileNotFoundError(f\"Dataset file not found: {DATASET_FILENAME}\")\n",
    "df = pd.read_csv(DATASET_FILENAME)\n",
    "\n",
    "# ensure required columns\n",
    "required = ['Region'] + FEATURES_X + FEATURES_C_CONT + FEATURES_C_CAT\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in data: {missing}\")\n",
    "\n",
    "# label‐encode categorical context\n",
    "le_study = LabelEncoder().fit(df['Study_ID'])\n",
    "df['Study_IDX'] = le_study.transform(df['Study_ID'])\n",
    "n_studies = len(le_study.classes_)\n",
    "\n",
    "le_drug = LabelEncoder().fit(df['Drug'])\n",
    "df['Drug_IDX'] = le_drug.transform(df['Drug'])\n",
    "n_drugs = len(le_drug.classes_)\n",
    "\n",
    "# split regions\n",
    "df_eu = df[df['Region']=='EU'].copy()\n",
    "df_us = df[df['Region']=='US'].copy()\n",
    "if df_eu.empty or df_us.empty:\n",
    "    raise ValueError(\"EU or US split has zero rows.\")\n",
    "\n",
    "# scale continuous context and x‐features\n",
    "scaler_c = StandardScaler().fit(df[FEATURES_C_CONT])\n",
    "scaler_x = StandardScaler().fit(df[FEATURES_X])\n",
    "\n",
    "# transform\n",
    "def to_numpy(subdf):\n",
    "    x_np     = scaler_x.transform(subdf[FEATURES_X])\n",
    "    cont_np  = scaler_c.transform(subdf[FEATURES_C_CONT])\n",
    "    study_np = subdf['Study_IDX'].to_numpy(dtype='int32')\n",
    "    drug_np  = subdf['Drug_IDX'].to_numpy(dtype='int32')\n",
    "    return x_np.astype('float32'), cont_np.astype('float32'), study_np, drug_np\n",
    "\n",
    "x_eu_np, c_eu_np, s_eu_np, d_eu_np = to_numpy(df_eu)\n",
    "x_us_np, c_us_np, s_us_np, d_us_np = to_numpy(df_us)\n",
    "\n",
    "# build tf.data pipelines\n",
    "ds_eu = tf.data.Dataset.from_tensor_slices((x_eu_np, c_eu_np, s_eu_np, d_eu_np))\\\n",
    "    .shuffle( max(10000, len(x_eu_np)) )\\\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "ds_us = tf.data.Dataset.from_tensor_slices((x_us_np, c_us_np, s_us_np, d_us_np))\\\n",
    "    .shuffle( max(10000, len(x_us_np)) )\\\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_ds = tf.data.Dataset.zip((ds_eu, ds_us)).prefetch(tf.data.AUTOTUNE).repeat()\n",
    "steps_per_epoch = min(len(x_eu_np), len(x_us_np)) // BATCH_SIZE\n",
    "if steps_per_epoch == 0:\n",
    "    raise ValueError(\"Not enough data for one batch.\")\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "# Build Models & Checkpoints\n",
    "\n",
    "input_dim_x = len(FEATURES_X)\n",
    "cont_dim    = len(FEATURES_C_CONT)\n",
    "\n",
    "G_EU2US = build_generator_cGAN(\n",
    "    input_dim_x, cont_dim,\n",
    "    n_studies, n_drugs,\n",
    "    STUDY_EMBED_DIM, DRUG_EMBED_DIM, Z_DIM,\n",
    "    num_residual=6, layer_width=128\n",
    ")\n",
    "G_US2EU = build_generator_cGAN(\n",
    "    input_dim_x, cont_dim,\n",
    "    n_studies, n_drugs,\n",
    "    STUDY_EMBED_DIM, DRUG_EMBED_DIM, Z_DIM,\n",
    "    num_residual=6, layer_width=128\n",
    ")\n",
    "D_EU = build_discriminator_cGAN(\n",
    "    input_dim_x, cont_dim,\n",
    "    n_studies, n_drugs,\n",
    "    STUDY_EMBED_DIM, DRUG_EMBED_DIM,\n",
    "    num_layers=4, start_width=128, name_suffix=\"_EU\"\n",
    ")\n",
    "D_US = build_discriminator_cGAN(\n",
    "    input_dim_x, cont_dim,\n",
    "    n_studies, n_drugs,\n",
    "    STUDY_EMBED_DIM, DRUG_EMBED_DIM,\n",
    "    num_layers=4, start_width=128, name_suffix=\"_US\"\n",
    ")\n",
    "\n",
    "gen_opt  = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "disc_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "mae_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "# checkpoint\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "ckpt = tf.train.Checkpoint(\n",
    "    G_EU2US=G_EU2US, G_US2EU=G_US2EU,\n",
    "    D_EU=D_EU, D_US=D_US,\n",
    "    gen_opt=gen_opt, disc_opt=disc_opt,\n",
    "    epoch=tf.Variable(0)\n",
    ")\n",
    "ckpt_mgr = tf.train.CheckpointManager(ckpt, CHECKPOINT_DIR, max_to_keep=5)\n",
    "if ckpt_mgr.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_mgr.latest_checkpoint).expect_partial()\n",
    "    print(\"Restored from\", ckpt_mgr.latest_checkpoint)\n",
    "initial_epoch = int(ckpt.epoch.numpy())\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "# Training\n",
    "\n",
    "@tf.function\n",
    "def train_step(eu_batch, us_batch):\n",
    "    x_eu, c_eu, s_eu, d_eu = eu_batch\n",
    "    x_us, c_us, s_us, d_us = us_batch\n",
    "    bs = tf.shape(x_eu)[0]\n",
    "    z_eu = tf.random.normal((bs, Z_DIM))\n",
    "    z_us = tf.random.normal((bs, Z_DIM))\n",
    "    z0   = tf.zeros((bs, Z_DIM))\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # forward\n",
    "        fake_us = G_EU2US([x_eu, c_eu, s_eu, d_eu, z_eu], training=True)\n",
    "        fake_eu = G_US2EU([x_us, c_us, s_us, d_us, z_us], training=True)\n",
    "        # cycle (use zero‐noise for cycle/identity)\n",
    "        cycled_eu = G_US2EU([fake_us, c_eu, s_eu, d_eu, z0], training=True)\n",
    "        cycled_us = G_EU2US([fake_eu, c_us, s_us, d_us, z0], training=True)\n",
    "        same_eu   = G_US2EU([x_eu, c_eu, s_eu, d_eu, z0], training=True)\n",
    "        same_us   = G_EU2US([x_us, c_us, s_us, d_us, z0], training=True)\n",
    "\n",
    "        # adversarial loss\n",
    "        D_US_real = D_US([x_us, c_us, s_us, d_us], training=True)\n",
    "        D_US_fake = D_US([fake_us, c_eu, s_eu, d_eu], training=True)\n",
    "        D_EU_real = D_EU([x_eu, c_eu, s_eu, d_eu], training=True)\n",
    "        D_EU_fake = D_EU([fake_eu, c_us, s_us, d_us], training=True)\n",
    "\n",
    "        loss_G_adv = (\n",
    "            mse_loss(tf.ones_like(D_US_fake), D_US_fake) +\n",
    "            mse_loss(tf.ones_like(D_EU_fake), D_EU_fake)\n",
    "        )\n",
    "        loss_cycle = mae_loss(x_eu, cycled_eu) + mae_loss(x_us, cycled_us)\n",
    "        loss_id    = mae_loss(x_eu, same_eu) + mae_loss(x_us, same_us)\n",
    "        total_G    = loss_G_adv + LAMBDA_CYCLE * loss_cycle + LAMBDA_ID * loss_id\n",
    "\n",
    "        loss_D_US = 0.5 * (\n",
    "            mse_loss(tf.ones_like(D_US_real), D_US_real) +\n",
    "            mse_loss(tf.zeros_like(D_US_fake), D_US_fake)\n",
    "        )\n",
    "        loss_D_EU = 0.5 * (\n",
    "            mse_loss(tf.ones_like(D_EU_real), D_EU_real) +\n",
    "            mse_loss(tf.zeros_like(D_EU_fake), D_EU_fake)\n",
    "        )\n",
    "        total_D = loss_D_US + loss_D_EU\n",
    "\n",
    "    grads_G = tape.gradient(total_G, G_EU2US.trainable_variables + G_US2EU.trainable_variables)\n",
    "    grads_D = tape.gradient(total_D, D_US.trainable_variables + D_EU.trainable_variables)\n",
    "    gen_opt.apply_gradients(zip(grads_G, G_EU2US.trainable_variables + G_US2EU.trainable_variables))\n",
    "    disc_opt.apply_gradients(zip(grads_D, D_US.trainable_variables + D_EU.trainable_variables))\n",
    "    return {\n",
    "        \"G_adv\": loss_G_adv, \"cycle_loss\": loss_cycle,\n",
    "        \"id_loss\": loss_id, \"D_loss\": total_D\n",
    "    }\n",
    "\n",
    "train_iter = iter(train_ds)\n",
    "for epoch in range(initial_epoch, TOTAL_EPOCHS):\n",
    "    start = time.time()\n",
    "    epoch_losses = {\"G_adv\":0., \"cycle_loss\":0., \"id_loss\":0., \"D_loss\":0.}\n",
    "    for step in range(steps_per_epoch):\n",
    "        eu_b, us_b = next(train_iter)\n",
    "        losses = train_step(eu_b, us_b)\n",
    "        for k in epoch_losses: epoch_losses[k] += losses[k]\n",
    "    # average\n",
    "    for k in epoch_losses:\n",
    "        epoch_losses[k] /= steps_per_epoch\n",
    "    print(f\"Epoch {epoch+1}/{TOTAL_EPOCHS} | \"\n",
    "          f\"G_adv={epoch_losses['G_adv']:.4f} \"\n",
    "          f\"cycle={epoch_losses['cycle_loss']:.4f} \"\n",
    "          f\"id={epoch_losses['id_loss']:.4f} \"\n",
    "          f\"D={epoch_losses['D_loss']:.4f} \"\n",
    "          f\"({time.time()-start:.1f}s)\")\n",
    "    # checkpoint\n",
    "    if (epoch+1) % SAVE_FREQ_EPOCHS == 0:\n",
    "        ckpt.epoch.assign(epoch+1)\n",
    "        p = ckpt_mgr.save()\n",
    "        print(\"Saved checkpoint:\", p)\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "# Generate Fake Translations\n",
    "\n",
    "n_eval = min(1000, x_eu_np.shape[0])\n",
    "x_eu_sub   = x_eu_np[:n_eval]\n",
    "c_eu_sub   = c_eu_np[:n_eval]\n",
    "s_eu_sub   = s_eu_np[:n_eval]\n",
    "d_eu_sub   = d_eu_np[:n_eval]\n",
    "z_eval     = np.random.randn(n_eval, Z_DIM).astype('float32')\n",
    "fake_us_np = G_EU2US.predict([x_eu_sub, c_eu_sub, s_eu_sub, d_eu_sub, z_eval])\n",
    "\n",
    "real_us_np = x_us_np[:n_eval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# 1) Subsample both domains\n",
    "n_eval = min(1000, x_eu_np.shape[0], x_us_np.shape[0])\n",
    "x_eu_sub = x_eu_np[:n_eval]\n",
    "c_eu_sub = c_eu_np[:n_eval]\n",
    "s_eu_sub = s_eu_np[:n_eval]\n",
    "d_eu_sub = d_eu_np[:n_eval]\n",
    "x_us_sub = x_us_np[:n_eval]\n",
    "c_us_sub = c_us_np[:n_eval]\n",
    "s_us_sub = s_us_np[:n_eval]\n",
    "d_us_sub = d_us_np[:n_eval]\n",
    "\n",
    "# 2) Generate fake US and fake EU\n",
    "z_eval = np.random.randn(n_eval, Z_DIM).astype('float32')\n",
    "fake_us_np = G_EU2US.predict([x_eu_sub, c_eu_sub, s_eu_sub, d_eu_sub, z_eval])\n",
    "fake_eu_np = G_US2EU.predict([x_us_sub, c_us_sub, s_us_sub, d_us_sub, z_eval])\n",
    "real_us_np = x_us_sub\n",
    "real_eu_np = x_eu_sub\n",
    "\n",
    "# 3) MMD helper\n",
    "def gaussian_kernel(X, Y, sigma=1.0):\n",
    "    XX = np.sum(X**2, axis=1).reshape(-1,1)\n",
    "    YY = np.sum(Y**2, axis=1).reshape(1,-1)\n",
    "    d2 = XX - 2*X.dot(Y.T) + YY\n",
    "    return np.exp(-d2/(2*sigma**2))\n",
    "\n",
    "def compute_mmd(X, Y, sigma=1.0):\n",
    "    Kxx = gaussian_kernel(X, X, sigma)\n",
    "    Kyy = gaussian_kernel(Y, Y, sigma)\n",
    "    Kxy = gaussian_kernel(X, Y, sigma)\n",
    "    return Kxx.mean() + Kyy.mean() - 2*Kxy.mean()\n",
    "\n",
    "# 4) Feature‐wise KDE plots (first 6 features)\n",
    "nplots = min(6, len(FEATURES_X))\n",
    "fig, axes = plt.subplots(nplots, 2, figsize=(12, 3*nplots))\n",
    "for i, feat in enumerate(FEATURES_X[:nplots]):\n",
    "    ax_eu, ax_us = axes[i]\n",
    "    sns.kdeplot(real_eu_np[:,i],    label='real EU', ax=ax_eu, fill=True)\n",
    "    sns.kdeplot(fake_eu_np[:,i],    label='fake EU', ax=ax_eu, fill=True)\n",
    "    ax_eu.set_title(f'EU Reconst.: {feat}')\n",
    "    sns.kdeplot(real_us_np[:,i],    label='real US', ax=ax_us, fill=True)\n",
    "    sns.kdeplot(fake_us_np[:,i],    label='fake US', ax=ax_us, fill=True)\n",
    "    ax_us.set_title(f'US Trans.: {feat}')\n",
    "    ax_eu.legend(); ax_us.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5) PCA 2D projection\n",
    "def plot_pca2d(datasets, labels, title):\n",
    "    pca = PCA(n_components=2)\n",
    "    Y = pca.fit_transform(np.vstack(datasets))\n",
    "    splits = np.cumsum([0]+[len(d) for d in datasets])\n",
    "    plt.figure(figsize=(6,5))\n",
    "    for idx, lab in enumerate(labels):\n",
    "        sl = slice(splits[idx], splits[idx+1])\n",
    "        plt.scatter(Y[sl,0], Y[sl,1], s=15, alpha=0.6, label=lab)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('PC1'); plt.ylabel('PC2')\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_pca2d([real_eu_np, fake_eu_np], ['real EU','fake EU'], 'PCA: EU real vs fake')\n",
    "plot_pca2d([real_us_np, fake_us_np], ['real US','fake US'], 'PCA: US real vs fake')\n",
    "\n",
    "# 6) Correlation‐matrix heatmaps for US domain\n",
    "real_corr_us = np.corrcoef(real_us_np, rowvar=False)\n",
    "fake_corr_us = np.corrcoef(fake_us_np, rowvar=False)\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(10,4))\n",
    "sns.heatmap(real_corr_us, vmin=-1, vmax=1, cmap='coolwarm', ax=ax1, cbar=False)\n",
    "ax1.set_title('Real US Corr')\n",
    "sns.heatmap(fake_corr_us, vmin=-1, vmax=1, cmap='coolwarm', ax=ax2, cbar=False)\n",
    "ax2.set_title('Fake US Corr')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7) NN‐distance histogram (privacy proxy)\n",
    "nbr = NearestNeighbors(n_neighbors=1).fit(real_us_np)\n",
    "dists, _ = nbr.kneighbors(fake_us_np)\n",
    "dists = dists.ravel()\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(dists, bins=50, kde=True)\n",
    "plt.title('NN distance: fake US → nearest real US')\n",
    "plt.xlabel('Euclidean dist'); plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"NN distances  mean={dists.mean():.3f}, med={np.median(dists):.3f}, min={dists.min():.3f}, max={dists.max():.3f}\")\n",
    "\n",
    "# 8) MMD vs σ\n",
    "sigmas = [0.5, 1.0, 2.0, 5.0]\n",
    "mmd_vals = [compute_mmd(real_us_np, fake_us_np, sigma=s) for s in sigmas]\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(sigmas, mmd_vals, '-o')\n",
    "plt.title('MMD(real US, fake US) vs σ')\n",
    "plt.xlabel('σ'); plt.ylabel('MMD')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional CycleGAN Evaluation\n",
    "\n",
    "import numpy as np, pandas as pd, scipy.stats as stats\n",
    "from   sklearn.linear_model   import LogisticRegression\n",
    "from   sklearn.metrics        import accuracy_score, roc_auc_score, mean_squared_error\n",
    "from   sklearn.neighbors      import NearestNeighbors\n",
    "\n",
    "\n",
    "real_eu_np   = x_eu_np\n",
    "cont_eu_np   = c_eu_np\n",
    "study_eu_idx = s_eu_np\n",
    "drug_eu_idx  = d_eu_np\n",
    "\n",
    "real_us_np   = x_us_np\n",
    "cont_us_np   = c_us_np\n",
    "study_us_idx = s_us_np\n",
    "drug_us_idx  = d_us_np\n",
    "\n",
    "try:\n",
    "    df_us\n",
    "    df_eu\n",
    "except NameError:\n",
    "    df_us = df[df['Region'] == 'US'].copy()\n",
    "    df_eu = df[df['Region'] == 'EU'].copy()\n",
    "\n",
    "def ks_test(real, fake, feature_names):\n",
    "    rows = []\n",
    "    for i, feat in enumerate(feature_names):\n",
    "        ks, p = stats.ks_2samp(real[:, i], fake[:, i])\n",
    "        rows.append((feat, ks, p))\n",
    "    return pd.DataFrame(rows, columns=['feature', 'ks_stat', 'p_value'])\n",
    "\n",
    "def chi2_categorical(real_df, fake_df, columns):\n",
    "    \"\"\"\n",
    "    χ² homogeneity test for each categorical column.\n",
    "    Uses chi2_contingency (2×k table) – never throws the\n",
    "    'observed / expected sums differ' error.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for col in columns:\n",
    "        cats = pd.Index(real_df[col].unique()).union(fake_df[col].unique())\n",
    "        real_counts = real_df[col].value_counts().reindex(cats, fill_value=0)\n",
    "        fake_counts = fake_df[col].value_counts().reindex(cats, fill_value=0)\n",
    "        table = np.vstack([real_counts.values, fake_counts.values])\n",
    "        chi2, p, _, _ = stats.chi2_contingency(table, correction=False)\n",
    "        rows.append((col, chi2, p))\n",
    "    return pd.DataFrame(rows, columns=['column', 'chi2_stat', 'p_value'])\n",
    "\n",
    "def corr_frobenius(r, f):\n",
    "    return np.linalg.norm(np.corrcoef(r, rowvar=False) -\n",
    "                          np.corrcoef(f, rowvar=False))\n",
    "\n",
    "def _rbf(X, Y, s=1.0):\n",
    "    XX = np.sum(X*X, 1)[:, None]\n",
    "    YY = np.sum(Y*Y, 1)[None, :]\n",
    "    return np.exp(-(XX - 2*X@Y.T + YY) / (2*s*s))\n",
    "\n",
    "def mmd(X, Y, s=1.0):\n",
    "    return _rbf(X, X, s).mean() + _rbf(Y, Y, s).mean() - 2*_rbf(X, Y, s).mean()\n",
    "\n",
    "def domain_confusion(real_src, real_tgt, fake_tgt):\n",
    "    X = np.vstack([real_src, real_tgt])\n",
    "    y = np.r_[np.zeros(len(real_src)), np.ones(len(real_tgt))]\n",
    "    clf = LogisticRegression(max_iter=1000).fit(X, y)\n",
    "    return (clf.predict(fake_tgt) == 1).mean()\n",
    "\n",
    "def tstr(fake_df, real_df, feats, label):\n",
    "    clf = LogisticRegression(max_iter=1000).fit(fake_df[feats], fake_df[label])\n",
    "    yhat = clf.predict(real_df[feats])\n",
    "    acc  = accuracy_score(real_df[label], yhat)\n",
    "    try:\n",
    "        auc  = roc_auc_score(real_df[label],\n",
    "                             clf.predict_proba(real_df[feats])[:, 1])\n",
    "    except ValueError:\n",
    "        auc = np.nan\n",
    "    return acc, auc\n",
    "\n",
    "def nn_stats(real, fake):\n",
    "    nbr = NearestNeighbors(n_neighbors=1).fit(real)\n",
    "    d, _ = nbr.kneighbors(fake)\n",
    "    d = d.ravel()\n",
    "    return {\"mean\": d.mean(), \"median\": np.median(d),\n",
    "            \"min\":  d.min(),  \"max\":    d.max()}\n",
    "\n",
    "def cycle_id(G, F, real, cont, s_idx, d_idx, zdim):\n",
    "    bs  = real.shape[0]\n",
    "    z_r = np.random.randn(bs, zdim).astype('float32')\n",
    "    z_0 = np.zeros((bs, zdim), dtype='float32')\n",
    "    fake   = G.predict([real, cont, s_idx, d_idx, z_r], verbose=0)\n",
    "    cycled = F.predict([fake, cont, s_idx, d_idx, z_0], verbose=0)\n",
    "    ident  = F.predict([real, cont, s_idx, d_idx, z_0], verbose=0)\n",
    "    return (mean_squared_error(real, cycled),\n",
    "            mean_squared_error(real, ident))\n",
    "\n",
    "# Generate samples\n",
    "\n",
    "z_us  = np.random.randn(len(real_eu_np), Z_DIM).astype('float32')\n",
    "fake_us_np = G_EU2US.predict([real_eu_np, cont_eu_np,\n",
    "                              study_eu_idx, drug_eu_idx, z_us],\n",
    "                             verbose=0)\n",
    "\n",
    "z_eu  = np.random.randn(len(real_us_np), Z_DIM).astype('float32')\n",
    "fake_eu_np = G_US2EU.predict([real_us_np, cont_us_np,\n",
    "                              study_us_idx, drug_us_idx, z_eu],\n",
    "                             verbose=0)\n",
    "\n",
    "\n",
    "df_fake_us = pd.DataFrame(fake_us_np, columns=FEATURES_X)\n",
    "\n",
    "# Recover categorical columns from the EU rows we just translated\n",
    "df_fake_us['Study_ID'] = df_eu['Study_ID'].values[:len(df_fake_us)]\n",
    "df_fake_us['Drug']     = df_eu['Drug'].values[:len(df_fake_us)]\n",
    "\n",
    "# Attach labels for TSTR\n",
    "df_fake_us[['Responder_Status', 'Remission_Status']] = \\\n",
    "    df_us[['Responder_Status', 'Remission_Status']]\\\n",
    "        .iloc[:len(df_fake_us)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"\\n================  E V A L U A T I O N  =================\\n\")\n",
    "\n",
    "# a) Univariate KS\n",
    "print(\"Univariate KS tests (US domain):\")\n",
    "print(ks_test(real_us_np, fake_us_np, FEATURES_X).to_string(index=False))\n",
    "\n",
    "# b) χ² on categorical context\n",
    "print(\"\\nCategorical χ² tests (US domain):\")\n",
    "print(chi2_categorical(df_us, df_fake_us, ['Study_ID', 'Drug']).to_string(index=False))\n",
    "\n",
    "# c) Correlation / MMD\n",
    "print(f\"\\nCorrelation Frobenius norm (US): {corr_frobenius(real_us_np, fake_us_np):.5f}\")\n",
    "print(f\"MMD (σ=1) (US)                  : {mmd(real_us_np, fake_us_np):.6f}\")\n",
    "\n",
    "# d) Domain confusion\n",
    "print(f\"\\nDomain classifier – fake‑US recognised as US: \"\n",
    "      f\"{domain_confusion(real_eu_np, real_us_np, fake_us_np):.3f}\")\n",
    "\n",
    "# e) TSTR utility\n",
    "acc, auc = tstr(df_fake_us, df_us, FEATURES_X, 'Responder_Status')\n",
    "print(f\"\\nTSTR Responder_Status  acc={acc:.3f}, AUC={auc:.3f}\")\n",
    "\n",
    "# f) Privacy proxy\n",
    "print(\"\\nNearest‑Neighbour distance fake‑US ➜ real‑US:\")\n",
    "for k, v in nn_stats(real_us_np, fake_us_np).items():\n",
    "    print(f\"  {k:<6s}: {v:.4f}\")\n",
    "\n",
    "# g) Cycle & identity consistency\n",
    "mse_cycle_us, mse_id_us = cycle_id(G_EU2US, G_US2EU,\n",
    "                                   real_eu_np, cont_eu_np,\n",
    "                                   study_eu_idx, drug_eu_idx, Z_DIM)\n",
    "mse_cycle_eu, mse_id_eu = cycle_id(G_US2EU, G_EU2US,\n",
    "                                   real_us_np, cont_us_np,\n",
    "                                   study_us_idx, drug_us_idx, Z_DIM)\n",
    "\n",
    "print(f\"\\nEU→US→EU cycle MSE : {mse_cycle_us:.5f}   |   EU identity MSE : {mse_id_us:.5f}\")\n",
    "print(f\"US→EU→US cycle MSE : {mse_cycle_eu:.5f}   |   US identity MSE : {mse_id_eu:.5f}\")\n",
    "\n",
    "print(\"\\n================  E N D   O F   E V A L  ================\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "510p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
